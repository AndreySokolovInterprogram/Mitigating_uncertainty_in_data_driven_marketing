{"cells":[{"cell_type":"markdown","source":["To start, press \"*Runtime*\" and press \"*Run all*\" on the **free*** instance of the Tesla T4 Google Colab!\n","\n","To install Unsloth on your computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda ).\n","\n","You will learn how to perform [data preparation](#Data), how to [Train](#Train), how to [run the model](#Inference) and [how to save it](#Save) (for example, for Llama.cpp )."],"metadata":{"id":"IqM-T1RTzY6C"}},{"cell_type":"markdown","source":["### Loading libraries"],"metadata":{"id":"rbv3wIXkQpkf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eSvM9zX_2d3"},"outputs":[],"source":["%%capture\n","# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"]},{"cell_type":"markdown","source":["### Loading the model"],"metadata":{"id":"r2v_X2fA0Df5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmUBVEnvCDJv","executionInfo":{"status":"ok","timestamp":1756910703219,"user_tz":-180,"elapsed":23333,"user":{"displayName":"–ê–ª–µ–∫—Å–µ–π –®—É—Ç–∫–∏–Ω","userId":"13719301666650930249"}},"outputId":"3fee1e81-324d-423b-d1f5-6a2b727d38c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.8.10: Fast Qwen2 patching. Transformers: 4.55.4.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# The name of the models to download\n","models = [\n","    \"lightblue/suzume-llama-3-8B-multilingual-orpo-borda-half\",\n","    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n","    \"unsloth/Qwen2-7B-Instruct-bnb-4bit\",\n","    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n","] # Try more models at https://huggingface.co/models\n","n=2 # Select the model number\n","model_name = models[n].split('/')[1]\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = models[n], # Reminder we support ANY Hugging Face model!\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")"]},{"cell_type":"markdown","source":["### Parameters for changing the LoRA model (We now add LoRA adapters so we only need to update 1 to 10% of all parameters!)"],"metadata":{"id":"SXd9bTZd1aaL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bZsfBuZDeCL"},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 1,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"markdown","source":["## Preparing a dataset for training"],"metadata":{"id":"vITh0KVJ10qX"}},{"cell_type":"markdown","source":["### Classification of posts"],"metadata":{"id":"wVkcOYrQ6b1I"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","import requests\n","import os\n","\n","# 10.5281/zenodo.17054610\n","record_id = '17054610'\n","filename_to_download = 'dataset_posts_tommyfish.xlsx'\n","\n","api_url = f'https://zenodo.org/api/records/{record_id}'\n","\n","try:\n","    print(f\"Searching for file '{filename_to_download}' in Zenodo repository with ID: {record_id}...\")\n","\n","    response = requests.get(api_url)\n","    response.raise_for_status()\n","    data = response.json()\n","\n","    file_url = None\n","    for file in data.get('files', []):\n","        if file.get('key') == filename_to_download:\n","            file_url = file.get('links', {}).get('self')\n","            break\n","\n","    if file_url:\n","        print(f\"File '{filename_to_download}' found. Starting download...\")\n","\n","        file_response = requests.get(file_url, stream=True)\n","        file_response.raise_for_status()\n","\n","        # Save the file to the current directory\n","        with open(filename_to_download, 'wb') as f:\n","            for chunk in file_response.iter_content(chunk_size=8192):\n","                f.write(chunk)\n","\n","        print(f\"File '{filename_to_download}' downloaded successfully.\")\n","    else:\n","        print(f\"File '{filename_to_download}' not found in the repository.\")\n","\n","except requests.exceptions.RequestException as e:\n","    print(f\"An error occurred while accessing Zenodo: {e}\")\n","\n","posts = pd.read_excel('dataset_posts_tommyfish.xlsx')\n","posts.drop(posts.index[posts['Post class']=='1,2,4'], inplace = True)\n","posts.reset_index(drop=True, inplace=True)\n","\n","nb_classes = 5\n","posts.dropna()\n","posts['Text'] = posts['Text'].astype(str)\n","\n","posts = posts[posts['Text'].notna()]\n","posts = posts[['Text','Post class']]\n","posts[\"Instruction\"] = \"\"\"You are an expert in analyzing VK group posts in the\n","        field of food delivery. You will be given the text of the group's post.\n","        Your task is to determine which class the group's post corresponds to.\n","        Answer only '0' if the post does not commit to anything (optional)\n","        Answer only '1' if the post obliges the group to a discount\n","        Answer only '2' if the post obliges the group to make a gift.\n","        Only answer '3' if the post obliges the group to give cashback.\n","        Answer only '4' if the post obliges the group to deliver the goods on\n","        time. Answer briefly, without explanation.\"\"\"\n","def split_dataframe(dataframe, test_proportion):\n","    total_size = len(dataframe)\n","    test_size = int(total_size * test_proportion)\n","    indices = np.arange(total_size)\n","    np.random.shuffle(indices)\n","    train_indices = indices[0:total_size-test_size]\n","    test_indices = indices[total_size - test_size:]\n","    return dataframe.iloc[train_indices], dataframe.iloc[test_indices]\n","\n","train, test = split_dataframe(posts, 0.3)\n","\n","from datasets import Dataset\n","ds = Dataset.from_dict({\"output\": train['Post class'],\"input\": train['Text'],'instruction':train['Instruction']})\n","ds[0]"],"metadata":{"id":"-zb0_7Jf6Rxp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756921700947,"user_tz":-180,"elapsed":6261,"user":{"displayName":"–ê–ª–µ–∫—Å–µ–π –®—É—Ç–∫–∏–Ω","userId":"13719301666650930249"}},"outputId":"a05a7a64-97ec-49fd-9254-1565f6f79182"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Searching for file 'dataset_posts_tommyfish.xlsx' in Zenodo repository with ID: 17048695...\n","File 'dataset_posts_tommyfish.xlsx' found. Starting download...\n","File 'dataset_posts_tommyfish.xlsx' downloaded successfully.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'output': 2,\n"," 'input': '–°–∫–æ—Ä–æ –î–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è? \\n–ê –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω —É–∂–µ —Å–µ–≥–æ–¥–Ω—è?üòª \\n \\n–û—Ç–º–µ—Ç—å—Ç–µ —ç—Ç–æ—Ç –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫ –≤ –∫—Ä—É–≥—É —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤ –∏ –±–ª–∏–∑–∫–∏—Ö! –ê –æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –≤–∫—É—Å–Ω–æ–≥–æ –∑–∞—Å—Ç–æ–ª—å—è –ø–æ–∑–∞–±–æ—Ç–∏–º—Å—è –º—ã‚ù§ \\n \\n–î–∞—Ä–∏–º –≤—Å–µ–º –∏–º–µ–Ω–∏–Ω–Ω–∏–∫–∞–º –ú–∏–Ω–∏ –°–µ—Ç! –ü—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º –∑–∞–∫–∞–∑–µ –æ—Ç 1200 —Ä—É–±–ª–µ–πüéÅ \\n \\nüç±–ß—Ç–æ –≤—Ö–æ–¥–∏—Ç –≤ —Å–æ—Å—Ç–∞–≤ —Å–µ—Ç–∞? \\n- –§–∏–ª–∞–¥–µ–ª—å—Ñ–∏—è –ª–∞–π—Ç —Å –∫—Ä–µ–≤–µ—Ç–∫–æ–π\\n- –ñ–∞—Ä–µ–Ω–Ω—ã–π —Ä–æ–ª–ª —Å –∫—É—Ä–æ—á–∫–æ–π\\n–ü—Ä–æ–º–æ–∫–æ–¥: –î–†\\n\\n**–ê–∫—Ü–∏—è –¥–µ–π—Å—Ç–≤—É–µ—Ç 3 –¥–Ω—è –¥–æ –∏ 3 –¥–Ω—è –ø–æ—Å–ª–µ —Å–æ–±—ã—Ç–∏—è\\n–î—Ä—É–∑—å—è, –Ω–∞–¥–µ–µ–º—Å—è —á—Ç–æ –í–∞—à–∏ –±—É–¥–Ω–∏ –ø—Ä–æ—Ö–æ–¥—è—Ç –≤–∫—É—Å–Ω–æ üòã \\n–ï—Å–ª–∏ —ç—Ç–æ –µ—â—ë –Ω–µ —Ç–∞–∫, —Ç–æ –í—ã –∑–Ω–∞–µ—Ç–µ, —á—Ç–æ –¥–µ–ª–∞—Ç—å ‚Äì —Å–∫–æ—Ä–µ–µ –ª–µ—Ç–∏—Ç–µ –≤ –Ω–∞—à–µ –º–µ–Ω—é http://vk.cc/c0vSiY –∏ –∑–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ –ª—é–±–∏–º—É—é –µ–¥—É üòç \\n–ß–µ–º –±—É–¥–µ—Ç–µ –±–∞–ª–æ–≤–∞—Ç—å —Å–µ–±—è —Å–µ–≥–æ–¥–Ω—è? üëçüèª',\n"," 'instruction': \"You are an expert in analyzing VK group posts in the\\n        field of food delivery. You will be given the text of the group's post.\\n        Your task is to determine which class the group's post corresponds to.\\n        Answer only '0' if the post does not commit to anything (optional)\\n        Answer only '1' if the post obliges the group to a discount\\n        Answer only '2' if the post obliges the group to make a gift.\\n        Only answer '3' if the post obliges the group to give cashback.\\n        Answer only '4' if the post obliges the group to deliver the goods on\\n        time. Answer briefly, without explanation.\"}"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["### Converting a dataset to a suitable format"],"metadata":{"id":"kkXvBogi6hiQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjY75GoYUCB8"},"outputs":[],"source":["alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"input\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","\n","from datasets import load_dataset\n","dataset = ds\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"]},{"cell_type":"markdown","source":["## Model training\n","Now we will use the `SFTTrainer` from Huggingface TRL! More documentation is here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer ). We do 60 steps for speed, but we can set `num_train_epochs=1` for a full run and disable `max_steps=None'. We also support the `DPOTrainer` from TRL!"],"metadata":{"id":"idAEIeSQ3xdS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"95_Nn-89DhsL","colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["ff395c22f0e74d279aa9475fe3772c3d","90ecf35cee2347cfa56e3b4c3d121252","a350da2d8ab246d1a9fcee6b03a39730","4ec40ae58e494ae2829535534d436760","487a4f99de4647c782375c5041636ac6","0103fb7825f341b7b3c5ae98a0de285c","42b862cc59bf4e99bc493a1c36add9d8","eb21e0dfc9cb40c6a8ab1368f67a9f73","944d67cde6de49a5be90c1e55cd7f85c","962a765c61eb4e979981e48e96d94fc7","b683f565e8e8437dbc0bfe2d09e0c9c8"]},"executionInfo":{"status":"ok","timestamp":1756910720633,"user_tz":-180,"elapsed":4219,"user":{"displayName":"–ê–ª–µ–∫—Å–µ–π –®—É—Ç–∫–∏–Ω","userId":"13719301666650930249"}},"outputId":"a41e4a10-72e3-4998-f12b-55ca6049bdd7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]}],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","\n","        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n","        warmup_steps = 5,\n","        max_steps = 60,\n","\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 5,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ejIt2xSNKKp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756910720680,"user_tz":-180,"elapsed":45,"user":{"displayName":"–ê–ª–µ–∫—Å–µ–π –®—É—Ç–∫–∏–Ω","userId":"13719301666650930249"}},"outputId":"620b4b62-dc8c-4757-8532-9876c17d6ad0"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU = Tesla T4. Max memory = 14.741 GB.\n","10.592 GB of memory reserved.\n"]}],"source":["#@title Indicators of available video memory\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqxqAZ7KJ4oL","colab":{"base_uri":"https://localhost:8080/","height":555},"outputId":"d7af5980-9b30-4f84-a372-e3f96f640a57","executionInfo":{"status":"ok","timestamp":1756911370356,"user_tz":-180,"elapsed":649673,"user":{"displayName":"–ê–ª–µ–∫—Å–µ–π –®—É—Ç–∫–∏–Ω","userId":"13719301666650930249"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 201 | Num Epochs = 3 | Total steps = 60\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n"," \"-____-\"     Trainable parameters = 40,370,176 of 7,655,986,688 (0.53% trained)\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [60/60 10:21, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>2.028800</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.515800</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.958100</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.955900</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.840600</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.862400</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.625400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.709900</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.662600</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.637700</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.577600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.633400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCqnaKmlO1U9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756911370368,"user_tz":-180,"elapsed":8,"user":{"displayName":"–ê–ª–µ–∫—Å–µ–π –®—É—Ç–∫–∏–Ω","userId":"13719301666650930249"}},"outputId":"47f8ee5e-b2f2-4b8a-872d-9cbbfb265252"},"outputs":[{"output_type":"stream","name":"stdout","text":["645.5671 seconds used for training.\n","10.76 minutes used for training.\n","Peak reserved memory = 10.592 GB.\n","Peak reserved memory for training = 0.0 GB.\n","Peak reserved memory % of max memory = 71.854 %.\n","Peak reserved memory for training % of max memory = 0.0 %.\n"]}],"source":["#@title Video memory usage indicators and statistics\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"markdown","source":["\n","## Launching the model"],"metadata":{"id":"ekOmTR1hSNcr"}},{"cell_type":"markdown","source":["### Classifying posts"],"metadata":{"id":"GL5Mt1fDuHwQ"}},{"cell_type":"code","source":["import re\n","\n","post_result = []\n","for text, class_post, Instruction in test.itertuples(index=False, name=None):\n","    # alpaca_prompt = Copied from above\n","    FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference!\n","    inputs = tokenizer(\n","    [\n","        alpaca_prompt.format(\n","            Instruction, # instruction\n","            text, # input\n","            \"\", # output - leave this blank for generation!\n","        )\n","    ], return_tensors = \"pt\").to(\"cuda\")\n","\n","    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","    message_answer =tokenizer.batch_decode(outputs)[0].rsplit('Response:', 1)[-1]\n","    match = re.search(r'[012345]', message_answer)\n","    if match:\n","        post_result.append(int(match.group()))\n","    else:\n","        post_result.append(-1)\n"],"metadata":{"id":"1_rOIHoFDptX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test[\"Answer\"] = post_result\n","df=test\n","# model_name=\"gte-Qwen2-7B-instruct\"\n","df.to_excel(model_name+'_output.xlsx', index=False)\n","df.to_csv(model_name+\"_output.csv\", index=False)\n","\n","df_difference = df.loc[df['Post class'] != df[\"Answer\"]]\n","#print(df_difference)\n","test.info()\n","df_difference.info()\n","df_difference.to_excel(model_name+'_difference.xlsx', index=False)\n","df_difference.to_csv(model_name+'_difference.csv')"],"metadata":{"id":"qrqfxAXCPFS8"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1r_R_0B_6lN20QvcDh--gOx-fgYPEpLMe","timestamp":1719299701893},{"file_id":"1zi_LFt1YKziqq0q1Aw3da125IrCTO51B","timestamp":1719065972064},{"file_id":"1OmZwhTkO-etVHqC8Y8XtAuwpuxVUWcTo","timestamp":1719054575762},{"file_id":"1ZBhzPeztjr4aQG5CH_PXIlPceTmGiPiF","timestamp":1719047954089},{"file_id":"1ouEw4En5FOx8ajeYlI5mlpEDCM1RLg-8","timestamp":1719039962052},{"file_id":"1nFL8AF-bXV-RKhN8KOzrCSvhjktbKtbN","timestamp":1718981823699},{"file_id":"1mvwsIQWDs2EdZxZQF9pRGnnOvE86MVvR","timestamp":1718974660136},{"file_id":"135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp","timestamp":1717692855151},{"file_id":"10NbwlsRChbma1v55m8LAPYG15uQv6HLo","timestamp":1713459337061},{"file_id":"1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_","timestamp":1708958229810},{"file_id":"1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5","timestamp":1703608159823},{"file_id":"1oW55fBmwzCOrBVX66RcpptL3a99qWBxb","timestamp":1702886138876}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}